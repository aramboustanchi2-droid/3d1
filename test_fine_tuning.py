"""
Test script for KURDO-AI Fine-Tuning capabilities
ØªØ³Øª Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Fine-Tuning Ø³ÛŒØ³ØªÙ… KURDO-AI
"""

from cad3d.super_ai.brain import SuperAIBrain
from cad3d.super_ai.fine_tuning import fine_tuning_manager

def test_fine_tuning_availability():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† Ø³ÛŒØ³ØªÙ… Fine-Tuning"""
    print("=" * 60)
    print("ðŸ” Testing Fine-Tuning Availability / ØªØ³Øª Ø¯Ø³ØªØ±Ø³ÛŒ")
    print("=" * 60)
    
    brain = SuperAIBrain()
    status = brain.get_status()
    
    print(f"âœ… Fine-Tuning Available: {status.get('fine_tuning', False)}")
    print(f"ðŸ“Š Previous Fine-Tuning Jobs: {status.get('fine_tuning_jobs', 0)}")
    
    if status.get('last_fine_tune'):
        print(f"ðŸ“… Last Fine-Tune: {status['last_fine_tune']}")
    
    print()
    return status.get('fine_tuning', False)

def test_architectural_corpus():
    """ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ"""
    print("=" * 60)
    print("ðŸ“š Testing Architectural Corpus / ØªØ³Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ")
    print("=" * 60)
    
    training_data = fine_tuning_manager.prepare_architectural_training_data()
    
    if training_data:
        print(f"âœ… Loaded {len(training_data)} training examples")
        print(f"ðŸ“ Sample example:")
        if len(training_data) > 0:
            sample = training_data[0]
            print(f"   System: {sample['messages'][0]['content'][:50]}...")
            print(f"   User: {sample['messages'][1]['content'][:50]}...")
            print(f"   Assistant: {sample['messages'][2]['content'][:50]}...")
    else:
        print("âš ï¸  No training data found. Check datasets/persian_corpus/ directory")
    
    print()
    return len(training_data) if training_data else 0

def test_anthropic_simulation():
    """ØªØ³Øª Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Fine-Tuning Ø¨Ø§ Anthropic"""
    print("=" * 60)
    print("ðŸ¤– Testing Anthropic Prompt Caching / ØªØ³Øª Anthropic")
    print("=" * 60)
    
    brain = SuperAIBrain()
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
    sample_data = [
        {
            "input": "Ø§Ù…Ú©Ø§Ù†â€ŒØ³Ù†Ø¬ÛŒ ÛŒÚ© Ø³Ø§Ø®ØªÙ…Ø§Ù† Û±Û° Ø·Ø¨Ù‚Ù‡",
            "output": "Ø¨Ø±Ø§ÛŒ Ø§Ù…Ú©Ø§Ù†â€ŒØ³Ù†Ø¬ÛŒ Ø¨Ø§ÛŒØ¯ Ø²Ù…ÛŒÙ†ØŒ Ù…Ù‚Ø±Ø±Ø§Øª Ùˆ Ø¨ÙˆØ¯Ø¬Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø´ÙˆØ¯"
        },
        {
            "input": "Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ±Ø§Ú©Ù… Ø³Ø§Ø®ØªÙ…Ø§Ù†ÛŒ",
            "output": "ØªØ±Ø§Ú©Ù… = Ù…Ø³Ø§Ø­Øª Ø²ÛŒØ±Ø¨Ù†Ø§ / Ù…Ø³Ø§Ø­Øª Ø²Ù…ÛŒÙ†"
        }
    ]
    
    result = brain.fine_tune_model(
        provider="anthropic",
        training_data=sample_data,
        use_architectural_corpus=False
    )
    
    print(f"Status: {result.get('status')}")
    print(f"Message: {result.get('message', 'N/A')}")
    
    if result.get('cached_prompt_file'):
        print(f"âœ… Cached prompt saved to: {result['cached_prompt_file']}")
    
    print()
    return result.get('status') == 'completed'

def test_openai_preparation():
    """ØªØ³Øª Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ OpenAI (Ø¨Ø¯ÙˆÙ† Ø§Ø¬Ø±Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ)"""
    print("=" * 60)
    print("ðŸš€ Testing OpenAI Preparation / ØªØ³Øª Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ OpenAI")
    print("=" * 60)
    
    import os
    from dotenv import load_dotenv
    load_dotenv()
    
    api_key = os.getenv("OPENAI_API_KEY")
    
    if api_key:
        print("âœ… OpenAI API Key found")
        print(f"   Key prefix: {api_key[:15]}...")
        print()
        print("âš ï¸  To actually start fine-tuning:")
        print("   brain.fine_tune_model(provider='openai', use_architectural_corpus=True)")
        print()
        print("ðŸ’° Estimated cost:")
        print("   ~100 examples Ã— 3 epochs = ~$1-2 USD")
    else:
        print("âŒ OpenAI API Key not found")
        print("   Add OPENAI_API_KEY to .env file")
        print("   Get key from: https://platform.openai.com/api-keys")
    
    print()
    return api_key is not None

def test_huggingface_availability():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Fine-Tune Ù…Ø­Ù„ÛŒ Ø¨Ø§ HuggingFace"""
    print("=" * 60)
    print("ðŸ¤— Testing HuggingFace Availability / ØªØ³Øª HuggingFace")
    print("=" * 60)
    
    try:
        import transformers
        import datasets
        print("âœ… transformers library installed")
        print(f"   Version: {transformers.__version__}")
        print()
        print("âœ… Ready for local fine-tuning!")
        print("   Example:")
        print("   brain.fine_tune_model(provider='huggingface', base_model='google/flan-t5-small')")
        available = True
    except ImportError:
        print("âŒ transformers library not installed")
        print("   Install with: pip install transformers datasets accelerate")
        available = False
    
    print()
    return available

def test_custom_data():
    """ØªØ³Øª Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÙØ§Ø±Ø´ÛŒ"""
    print("=" * 60)
    print("ðŸ“ Testing Custom Training Data / ØªØ³Øª Ø¯Ø§Ø¯Ù‡ Ø³ÙØ§Ø±Ø´ÛŒ")
    print("=" * 60)
    
    custom_data = [
        {
            "messages": [
                {"role": "system", "content": "ØªÙˆ ÛŒÚ© Ù…Ø´Ø§ÙˆØ± Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù‡Ø³ØªÛŒ"},
                {"role": "user", "content": "Ø¶Ø±ÛŒØ¨ Ø§Ø´ØºØ§Ù„ Ú†ÛŒØ³ØªØŸ"},
                {"role": "assistant", "content": "Ø¶Ø±ÛŒØ¨ Ø§Ø´ØºØ§Ù„ Ù†Ø³Ø¨Øª Ø³Ø·Ø­ Ø§Ø´ØºØ§Ù„ Ø²Ù…ÛŒÙ† Ø¨Ù‡ Ú©Ù„ Ù…Ø³Ø§Ø­Øª Ø²Ù…ÛŒÙ† Ø§Ø³Øª"}
            ]
        },
        {
            "messages": [
                {"role": "system", "content": "ØªÙˆ ÛŒÚ© Ù…Ø´Ø§ÙˆØ± Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù‡Ø³ØªÛŒ"},
                {"role": "user", "content": "ØªØ±Ø§Ú©Ù… Ø³Ø§Ø®ØªÙ…Ø§Ù†ÛŒ Ú†Ø·ÙˆØ± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ´Ù‡ØŸ"},
                {"role": "assistant", "content": "ØªØ±Ø§Ú©Ù… = Ù…Ø¬Ù…ÙˆØ¹ Ø²ÛŒØ±Ø¨Ù†Ø§Ù‡Ø§ / Ù…Ø³Ø§Ø­Øª Ø²Ù…ÛŒÙ†"}
            ]
        }
    ]
    
    print(f"âœ… Created {len(custom_data)} custom training examples")
    print()
    print("Sample format:")
    print(f"  {custom_data[0]}")
    print()
    print("ðŸ’¡ To use custom data:")
    print("   brain.fine_tune_model(provider='openai', training_data=custom_data)")
    
    print()
    return True

def show_fine_tuning_history():
    """Ù†Ù…Ø§ÛŒØ´ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Fine-Tuning"""
    print("=" * 60)
    print("ðŸ“œ Fine-Tuning History / ØªØ§Ø±ÛŒØ®Ú†Ù‡ Fine-Tuning")
    print("=" * 60)
    
    history = fine_tuning_manager.get_fine_tuning_history()
    
    if history:
        print(f"âœ… Found {len(history)} previous fine-tuning jobs:")
        print()
        for idx, job in enumerate(history, 1):
            print(f"{idx}. Provider: {job.get('provider')}")
            print(f"   Status: {job.get('status')}")
            print(f"   Date: {job.get('timestamp')}")
            if job.get('job_id'):
                print(f"   Job ID: {job.get('job_id')}")
            print()
    else:
        print("ðŸ“­ No fine-tuning history yet")
        print("   Run a fine-tuning job to see it here!")
    
    print()

def main():
    """Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù… ØªØ³Øªâ€ŒÙ‡Ø§"""
    print()
    print("ðŸŽ“" * 30)
    print("KURDO-AI FINE-TUNING TEST SUITE")
    print("Ù…Ø¬Ù…ÙˆØ¹Ù‡ ØªØ³Øª Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Fine-Tuning")
    print("ðŸŽ“" * 30)
    print()
    
    results = {}
    
    # Test 1: Availability
    results['availability'] = test_fine_tuning_availability()
    
    # Test 2: Architectural Corpus
    results['corpus_count'] = test_architectural_corpus()
    
    # Test 3: Anthropic Simulation
    results['anthropic'] = test_anthropic_simulation()
    
    # Test 4: OpenAI Preparation
    results['openai_ready'] = test_openai_preparation()
    
    # Test 5: HuggingFace
    results['huggingface'] = test_huggingface_availability()
    
    # Test 6: Custom Data
    results['custom_data'] = test_custom_data()
    
    # Show History
    show_fine_tuning_history()
    
    # Summary
    print("=" * 60)
    print("ðŸ“Š SUMMARY / Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬")
    print("=" * 60)
    print(f"Fine-Tuning Module: {'âœ… Available' if results['availability'] else 'âŒ Not Available'}")
    print(f"Architectural Corpus: {'âœ… ' + str(results['corpus_count']) + ' examples' if results['corpus_count'] > 0 else 'âš ï¸  No data'}")
    print(f"Anthropic Ready: {'âœ… Yes' if results['anthropic'] else 'âŒ No'}")
    print(f"OpenAI Ready: {'âœ… Yes' if results['openai_ready'] else 'âš ï¸  API key needed'}")
    print(f"HuggingFace Ready: {'âœ… Yes' if results['huggingface'] else 'âš ï¸  Install needed'}")
    print(f"Custom Data Format: {'âœ… Valid' if results['custom_data'] else 'âŒ Invalid'}")
    print()
    
    # Recommendations
    print("=" * 60)
    print("ðŸ’¡ RECOMMENDATIONS / ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§")
    print("=" * 60)
    
    if results['anthropic']:
        print("âœ… You can start with Anthropic (fast & free):")
        print("   brain.fine_tune_model(provider='anthropic', use_architectural_corpus=True)")
        print()
    
    if results['openai_ready']:
        print("âœ… OpenAI fine-tuning ready (costs ~$1-2):")
        print("   brain.fine_tune_model(provider='openai', use_architectural_corpus=True)")
        print()
    elif not results['openai_ready']:
        print("âš ï¸  Add OpenAI API key to .env for production fine-tuning")
        print()
    
    if results['huggingface']:
        print("âœ… Local fine-tuning available (free, but slower):")
        print("   brain.fine_tune_model(provider='huggingface', base_model='google/flan-t5-small')")
        print()
    
    if results['corpus_count'] > 0:
        print(f"âœ… {results['corpus_count']} architectural examples ready for training")
        print()
    
    print("=" * 60)
    print("ðŸŽ‰ Fine-Tuning System Ready! / Ø³ÛŒØ³ØªÙ… Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!")
    print("=" * 60)
    print()


if __name__ == "__main__":
    main()
