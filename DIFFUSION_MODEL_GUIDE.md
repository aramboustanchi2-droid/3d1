# 3D Diffusion Model for CAD Conversion

## Ù…Ø¯Ù„ Ø§Ù†ØªØ´Ø§Ø± Ø³Ù‡â€ŒØ¨Ø¹Ø¯ÛŒ - Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ØªØ±ÛŒÙ† Ø±ÙˆØ´ ØªÙˆÙ„ÛŒØ¯ 3D

Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ… Ø§Ø² **Diffusion Models** (Ù…Ø´Ø§Ø¨Ù‡ Stable Diffusion 3D, Point-E, DeepFloyd) Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù†Ù‚Ø´Ù‡â€ŒÙ‡Ø§ÛŒ 2D Ø¨Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ 3D Ø¯Ù‚ÛŒÙ‚ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

---

## ğŸš€ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ

### 1. **Architecture (Ù…Ø¹Ù…Ø§Ø±ÛŒ)**

- **DDPM** (Denoising Diffusion Probabilistic Models): Ø±ÙˆØ´ Ø§ØµÙ„ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§
- **DDIM Sampling**: ØªÙˆÙ„ÛŒØ¯ Ø³Ø±ÛŒØ¹ (10-50 Ú¯Ø§Ù… Ø¨Ù‡ Ø¬Ø§ÛŒ 1000 Ú¯Ø§Ù…)
- **PointNet++**: Ø¯Ø±Ú© Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø¨Ø±Ù†Ù‚Ø·Ù‡â€ŒÙ‡Ø§ÛŒ 3D
- **CLIP Image Encoder**: Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ 2D Ø¨Ø±Ø§ÛŒ Ù‡Ø¯Ø§ÛŒØª ØªÙˆÙ„ÛŒØ¯ 3D
- **U-Net 3D**: Ø´Ø¨Ú©Ù‡ Ø§ØµÙ„ÛŒ denoising Ø¨Ø§ attention mechanisms

### 2. **Vision Transformer Integration**

- ØªØ±Ú©ÛŒØ¨ ViT Ø¨Ø§ Diffusion Ø¨Ø±Ø§ÛŒ Ù‚Ø¯Ø±Øª Ø¨ÛŒØ´ØªØ±
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ (semantic)ØŒ Ø§Ø±ØªÙØ§Ø¹ (height)ØŒ Ø¹Ù…Ù‚ (depth)ØŒ Ùˆ Ù…ÙˆØ§Ø¯ (materials)
- Feature Fusion Layer Ø¨Ø±Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ViT Ùˆ Diffusion
- Ù†ØªÛŒØ¬Ù‡: **Ø¯Ù‚Øª Ùˆ Ø¬Ø²Ø¦ÛŒØ§Øª Ú†Ù†Ø¯ÛŒÙ† Ø¨Ø±Ø§Ø¨Ø± Ø¨ÛŒØ´ØªØ±**

### 3. **Continuous Learning (ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ø§ÙˆÙ…)**

- **Experience Replay Buffer**: Ø°Ø®ÛŒØ±Ù‡ ØªØ¨Ø¯ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±
- ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø² Ù‡Ø± ØªØ¨Ø¯ÛŒÙ„
- Ø¨Ù‡Ø¨ÙˆØ¯ ØªØ¯Ø±ÛŒØ¬ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡
- Ù‡Ø± 10 ØªØ¨Ø¯ÛŒÙ„ â†’ ÛŒÚ© Ø¨Ø§Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø¯Ù„

### 4. **Multi-Stage Training**

- Pre-training Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÛŒÙ†ØªØªÛŒÚ©
- Fine-tuning Ø±ÙˆÛŒ Ù†Ù‚Ø´Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ CAD
- Progressive resolution training
- Loss scheduling Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ù‡ÛŒÙ†Ù‡

---

## ğŸ“¦ Ù†ØµØ¨ Ùˆ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ

### Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§

```bash
pip install torch torchvision
pip install ezdxf opencv-python numpy scipy matplotlib
```

### Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø³Ø§Ø¯Ù‡

```python
from cad3d.hybrid_vit_diffusion import create_hybrid_converter

# Ø§ÛŒØ¬Ø§Ø¯ converter
converter = create_hybrid_converter(
    device="cuda",  # ÛŒØ§ "cpu"
    enable_learning=True  # ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ø§ÙˆÙ…
)

# ØªØ¨Ø¯ÛŒÙ„ ØªØµÙˆÛŒØ± Ø¨Ù‡ 3D
results = converter.convert_image_to_3d(
    image_path="plan.png",
    output_path="plan_3d.dxf",
    sampling_steps=50,  # ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø§Ù…â€ŒÙ‡Ø§ÛŒ sampling (Ú©Ù…ØªØ± = Ø³Ø±ÛŒØ¹â€ŒØªØ±)
    learn_from_conversion=True  # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø§ÛŒÙ† ØªØ¨Ø¯ÛŒÙ„
)
```

---

## ğŸ“ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„

### 1. Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ

```python
from cad3d.diffusion_trainer import create_synthetic_training_data

# ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÛŒÙ†ØªØªÛŒÚ© Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹
create_synthetic_training_data(
    output_dir="training_data/diffusion_synthetic",
    num_samples=500
)
```

**Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ:**

```
training_data/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ drawing_001.png
â”‚   â”œâ”€â”€ drawing_002.png
â”‚   â””â”€â”€ ...
â””â”€â”€ pointclouds/
    â”œâ”€â”€ drawing_001.npy  # (N, 3) numpy array
    â”œâ”€â”€ drawing_002.npy
    â””â”€â”€ ...
```

### 2. Ø¢Ù…ÙˆØ²Ø´ Ø§ÙˆÙ„ÛŒÙ‡

```python
from cad3d.diffusion_trainer import DiffusionTrainer, CAD2D3DDataset
from cad3d.diffusion_3d_model import create_diffusion_model

# Ø§ÛŒØ¬Ø§Ø¯ dataset
dataset = CAD2D3DDataset(
    data_dir="training_data/diffusion_synthetic",
    image_size=256,
    num_points=4096,
    augment=True
)

# Split train/val
train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(
    dataset, [train_size, val_size]
)

# Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
model = create_diffusion_model(
    num_points=4096,
    timesteps=1000,
    device="cuda"
)

# Ø§ÛŒØ¬Ø§Ø¯ trainer
trainer = DiffusionTrainer(
    model=model,
    device="cuda",
    learning_rate=1e-4
)

# Ø¢Ù…ÙˆØ²Ø´
trainer.train(
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    epochs=100,
    batch_size=8,
    save_every=10
)
```

### 3. Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ

```python
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ù‚Ø´Ù‡â€ŒÙ‡Ø§ÛŒ CAD ÙˆØ§Ù‚Ø¹ÛŒ
real_dataset = CAD2D3DDataset(
    data_dir="training_data/real_cad",
    image_size=256,
    num_points=4096,
    augment=True
)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ pre-trained
trainer.load_checkpoint("trained_models/diffusion/diffusion_best.pth")

# Fine-tuning
trainer.train(
    train_dataset=real_dataset,
    epochs=50,
    batch_size=4,
    save_every=5
)
```

---

## ğŸ”¬ Ù†Ù…Ø§ÛŒØ´ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´

```bash
# Ø§Ø¬Ø±Ø§ÛŒ demo Ú©Ø§Ù…Ù„
python demo_diffusion.py
```

Ø§ÛŒÙ† demo Ø´Ø§Ù…Ù„:

1. âœ… ØªØ¨Ø¯ÛŒÙ„ Ø³Ø§Ø¯Ù‡ ØªØµÙˆÛŒØ± Ø¨Ù‡ 3D
2. âœ… ØªØ¨Ø¯ÛŒÙ„ batch Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
3. âœ… Ù†Ù…Ø§ÛŒØ´ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ø§ÙˆÙ…
4. âœ… Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ sampling (DDPM vs DDIM)
5. âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„

---

## ğŸ¯ Pipeline Ú©Ø§Ù…Ù„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Image â”‚ (2D CAD Drawing)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vision Transformer  â”‚ Extract rich features:
â”‚ (ViT)               â”‚ - Semantic classes
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ - Height map
       â”‚                 - Depth map
       â”‚                 - Materials
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Fusion      â”‚ Combine ViT + CLIP features
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3D Diffusion Model  â”‚ Generate point cloud:
â”‚ (DDIM Sampling)     â”‚ - Start from noise
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ - Denoise step by step
       â”‚                 - Guided by 2D features
       â”‚                 - 10-50 steps
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Point Cloud         â”‚ (N, 3) 3D coordinates
â”‚ Enhancement         â”‚ + semantic colors
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ + height information
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DXF Mesh Export     â”‚ Convert to CAD format
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output 3D DXF       â”‚ âœ… Ready for CAD software
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼ (if learning enabled)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Experience Replay   â”‚ Store for learning
â”‚ Buffer              â”‚ Periodic model update
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±

| Ø±ÙˆØ´ | Ø¯Ù‚Øª | Ø³Ø±Ø¹Øª | ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ | Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ |
|-----|------|------|---------|----------|
| **Simple Extrusion** | â­â­ | â­â­â­â­â­ | âŒ | â­ |
| **Vision Transformer** | â­â­â­â­ | â­â­â­â­ | âœ… | â­â­â­ |
| **3D Diffusion** | â­â­â­â­â­ | â­â­â­ | âœ… | â­â­â­â­ |
| **Hybrid (ViT + Diffusion)** | â­â­â­â­â­ | â­â­â­ | âœ…âœ… | â­â­â­â­ |

### Ù…Ø²Ø§ÛŒØ§ÛŒ Diffusion Model

1. âœ… **Ø¬Ø²Ø¦ÛŒØ§Øª Ø¯Ù‚ÛŒÙ‚**: ØªÙˆÙ„ÛŒØ¯ geometry Ù¾ÛŒÚ†ÛŒØ¯Ù‡
2. âœ… **Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ÛŒ**: Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ Ù‡Ø± Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡
3. âœ… **Scalability**: Ø§Ø² simple ØªØ§ complex
4. âœ… **State-of-the-art**: Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ ÙØ¹Ù„ÛŒ Ø¯Ø± ØªØ­Ù‚ÛŒÙ‚Ø§Øª
5. âœ… **Continuous Learning**: Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡

---

## ğŸ”§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡

### Sampling Methods

#### DDPM (Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ØŒ Ú©Ù†Ø¯)

```python
# 1000 steps, maximum quality
point_cloud = diffusion.p_sample_loop(
    shape=(batch_size, 4096, 3),
    condition=features,
    device="cuda",
    progress=True
)
```

#### DDIM (Ø³Ø±ÛŒØ¹ØŒ Ú©ÛŒÙÛŒØª Ø®ÙˆØ¨)

```python
# 50 steps, 20x faster
point_cloud = diffusion.ddim_sample(
    shape=(batch_size, 4096, 3),
    condition=features,
    steps=50,
    eta=0.0,  # 0.0 = deterministic, 1.0 = stochastic
    device="cuda"
)
```

### ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø·

```python
# Ú©Ù…: Ø³Ø±ÛŒØ¹ØŒ Ú©Ù…â€ŒØ¬Ø²Ø¦ÛŒØ§Øª
model = create_diffusion_model(num_points=1024)

# Ù…ØªÙˆØ³Ø·: ØªÙˆØ§Ø²Ù† Ø®ÙˆØ¨
model = create_diffusion_model(num_points=2048)

# Ø²ÛŒØ§Ø¯: Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ø§Ù„Ø§ØŒ Ú©Ù†Ø¯ØªØ±
model = create_diffusion_model(num_points=8192)
```

### Learning Rate Schedule

```python
# Ø¨Ø±Ø§ÛŒ fine-tuning
trainer = DiffusionTrainer(
    model=model,
    learning_rate=1e-5  # Ú©Ù…ØªØ± Ø¨Ø±Ø§ÛŒ stability
)

# Ø¨Ø±Ø§ÛŒ training Ø§Ø² ØµÙØ±
trainer = DiffusionTrainer(
    model=model,
    learning_rate=1e-4  # Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ learning Ø³Ø±ÛŒØ¹
)
```

---

## ğŸ“ˆ Training Tips

### 1. Progressive Training

```python
# Ù…Ø±Ø­Ù„Ù‡ 1: Resolution Ù¾Ø§ÛŒÛŒÙ†ØŒ Ø³Ø±ÛŒØ¹
train_on_low_res(image_size=128, epochs=20)

# Ù…Ø±Ø­Ù„Ù‡ 2: Resolution Ù…ØªÙˆØ³Ø·
train_on_medium_res(image_size=256, epochs=30)

# Ù…Ø±Ø­Ù„Ù‡ 3: Resolution Ø¨Ø§Ù„Ø§
train_on_high_res(image_size=512, epochs=50)
```

### 2. Data Augmentation

```python
dataset = CAD2D3DDataset(
    data_dir="...",
    augment=True  # ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ augmentation:
                  # - Horizontal flip
                  # - Brightness/contrast
                  # - Rotation (optional)
)
```

### 3. Monitoring

```python
# Ø¨Ø±Ø±Ø³ÛŒ loss history
import matplotlib.pyplot as plt

plt.plot(trainer.loss_history)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Progress')
plt.savefig('training_loss.png')
```

---

## ğŸ¯ Use Cases

### 1. Architectural Floor Plans

```python
converter.convert_image_to_3d(
    image_path="floor_plan.png",
    output_path="building_3d.dxf",
    sampling_steps=50
)
# Ù†ØªÛŒØ¬Ù‡: Ø³Ø§Ø®ØªÙ…Ø§Ù† 3D Ø¨Ø§ Ø§ØªØ§Ù‚â€ŒÙ‡Ø§ØŒ Ø¯ÛŒÙˆØ§Ø±Ù‡Ø§ØŒ Ø¯Ø±Ù‡Ø§
```

### 2. Mechanical Parts

```python
converter.convert_image_to_3d(
    image_path="part_drawing.png",
    output_path="part_3d.dxf",
    sampling_steps=100  # More steps for precision
)
# Ù†ØªÛŒØ¬Ù‡: Ù‚Ø·Ø¹Ù‡ Ù…Ú©Ø§Ù†ÛŒÚ©ÛŒ Ø¯Ù‚ÛŒÙ‚
```

### 3. Landscape Design

```python
converter.convert_image_to_3d(
    image_path="landscape_plan.png",
    output_path="terrain_3d.dxf",
    sampling_steps=50
)
# Ù†ØªÛŒØ¬Ù‡: ØªÙˆÙ¾ÙˆÚ¯Ø±Ø§ÙÛŒØŒ Ø¯Ø±Ø®ØªØ§Ù†ØŒ Ù…Ø³ÛŒØ±Ù‡Ø§
```

---

## ğŸš€ Performance Optimization

### GPU Acceleration

```python
# Ø¨Ø±Ø±Ø³ÛŒ CUDA
if torch.cuda.is_available():
    device = "cuda"
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    device = "cpu"
    print("Using CPU (slower)")

converter = create_hybrid_converter(device=device)
```

### Batch Processing

```python
# Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú†Ù†Ø¯ÛŒÙ† ÙØ§ÛŒÙ„ Ù‡Ù…Ø²Ù…Ø§Ù†
image_paths = list(Path("input_images").glob("*.png"))

for img_path in image_paths:
    output_path = Path("output") / f"{img_path.stem}_3d.dxf"
    converter.convert_image_to_3d(img_path, output_path)
```

### Memory Management

```python
# Ø¨Ø±Ø§ÛŒ GPU Ø¨Ø§ memory Ú©Ù…
model = create_diffusion_model(
    num_points=2048,  # Ú©Ù…ØªØ± Ø§Ø² 4096
    timesteps=1000,
    device="cuda"
)

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² batch_size Ú©ÙˆÚ†Ú©
trainer.train(batch_size=2)  # Ø¨Ù‡ Ø¬Ø§ÛŒ 8
```

---

## ğŸ“š References

Ø§ÛŒÙ† implementation Ø§Ù„Ù‡Ø§Ù…â€ŒÚ¯Ø±ÙØªÙ‡ Ø§Ø²:

1. **DDPM** - Denoising Diffusion Probabilistic Models (Ho et al., 2020)
2. **DDIM** - Denoising Diffusion Implicit Models (Song et al., 2021)
3. **Point-E** - OpenAI's Point Cloud Diffusion (Nichol et al., 2022)
4. **Stable Diffusion** - Stability.ai (Rombach et al., 2022)
5. **PointNet++** - Deep Learning on Point Sets (Qi et al., 2017)
6. **DreamFusion** - Text-to-3D using 2D Diffusion (Poole et al., 2022)

---

## ğŸ’¡ Ù…Ø«Ø§Ù„ Ú©Ø§Ù…Ù„

```python
# 1. Import
from cad3d.hybrid_vit_diffusion import create_hybrid_converter

# 2. Create converter
converter = create_hybrid_converter(
    device="cuda",
    enable_learning=True
)

# 3. Convert single image
results = converter.convert_image_to_3d(
    image_path="my_plan.png",
    output_path="my_plan_3d.dxf",
    sampling_steps=50,
    learn_from_conversion=True
)

# 4. Check results
print(f"Generated {results['num_points']} points")
print(f"Time: {results['conversion_time']:.2f}s")
print(f"Learning updates: {results['learning_updates']}")

# 5. Open in CAD software
# my_plan_3d.dxf â†’ AutoCAD, FreeCAD, etc.
```

---

## âœ… Ø®Ù„Ø§ØµÙ‡

**3D Diffusion Model** Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ØªØ±ÛŒÙ† Ùˆ Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ´ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù†Ù‚Ø´Ù‡â€ŒÙ‡Ø§ÛŒ 2D Ø¨Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ 3D Ø§Ø³Øª.

### ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ

- ğŸ¯ **Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§**: Ø¬Ø²Ø¦ÛŒØ§Øª Ø¯Ù‚ÛŒÙ‚ Ùˆ realistic
- ğŸš€ **Ø³Ø±Ø¹Øª Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„**: Ø¨Ø§ DDIM sampling
- ğŸ§  **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯**: Ø¨Ù‡Ø¨ÙˆØ¯ Ù…Ø¯Ø§ÙˆÙ… Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡
- ğŸ”§ **Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±**: Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù‡Ø± use case
- ğŸ“¦ **Ø§Ø¯ØºØ§Ù… Ø¢Ø³Ø§Ù†**: API Ø³Ø§Ø¯Ù‡ Ùˆ ÙˆØ§Ø¶Ø­

### Ú†Ø±Ø§ Diffusion?

- âœ… State-of-the-art Ø¯Ø± ØªØ­Ù‚ÛŒÙ‚Ø§Øª AI
- âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Stable Diffusion, DALL-E, Midjourney
- âœ… Ù‚Ø§Ø¨Ù„ÛŒØª ØªÙˆÙ„ÛŒØ¯ Ø¬Ø²Ø¦ÛŒØ§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡
- âœ… Ù‚Ø§Ø¨Ù„ training Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÙØ§Ø±Ø´ÛŒ
- âœ… Continuous improvement Ø¨Ø§ experience replay

**Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ… Ù‚Ø¯Ø±Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø´Ù…Ø§ Ø±Ø§ ØµØ¯Ù‡Ø§ Ø¨Ø±Ø§Ø¨Ø± Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯! ğŸš€**
