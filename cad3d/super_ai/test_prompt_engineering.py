#!/usr/bin/env python3
"""
KURDO-AI Prompt Engineering - Test & Demo
Tests training-free methods: Templates, Few-shot, Chain-of-Thought, Caching
"""

import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from cad3d.super_ai.brain import SuperAIBrain
import json


def print_section(title):
    """Print formatted section header."""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80 + "\n")


def print_result(result):
    """Pretty print result."""
    if isinstance(result, str):
        print(result)
    else:
        print(json.dumps(result, indent=2, ensure_ascii=False))
    print()


def test_prompt_templates():
    """Test built-in prompt templates."""
    print_section("1. PROMPT TEMPLATES")
    
    brain = SuperAIBrain()
    
    # List all templates
    print("üìã Available Templates:")
    templates = brain.list_prompt_templates()
    for template in templates:
        print(f"  ‚Ä¢ {template}")
    
    print("\n" + "-" * 60 + "\n")
    
    # Test architectural calculation template
    print("üèóÔ∏è  Example 1: Architectural Calculation")
    print("-" * 60)
    prompt = brain.use_prompt_template(
        "arch_calculation",
        task="ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÖÿ≥ÿßÿ≠ÿ™ ÿßÿ™ÿßŸÇ",
        given_values="ÿ∑ŸàŸÑ: 6 ŸÖÿ™ÿ±ÿå ÿπÿ±ÿ∂: 4 ŸÖÿ™ÿ±",
        required_output="ŸÖÿ≥ÿßÿ≠ÿ™ ÿ®Ÿá ŸÖÿ™ÿ± ŸÖÿ±ÿ®ÿπ"
    )
    print(prompt)
    
    # Test code generation template
    print("\nüíª Example 2: Code Generation")
    print("-" * 60)
    prompt = brain.use_prompt_template(
        "code_generation",
        language="Python",
        task="Calculate room area",
        requirements="- Take length and width as input\n- Return area in square meters\n- Add input validation"
    )
    print(prompt)
    
    # Test design review template
    print("\nüîç Example 3: Design Review")
    print("-" * 60)
    prompt = brain.use_prompt_template(
        "design_review",
        project_name="Residential Tower - Tehran",
        design_element="Foundation design for 10-story building",
        applicable_standards="ŸÖÿ®ÿ≠ÿ´ 19ÿå ÿßÿ≥ÿ™ÿßŸÜÿØÿßÿ±ÿØ 2800"
    )
    print(prompt)


def test_few_shot_learning():
    """Test few-shot learning."""
    print_section("2. FEW-SHOT LEARNING")
    
    brain = SuperAIBrain()
    
    # Architectural examples
    examples = [
        {
            "input": "ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÖÿ≥ÿßÿ≠ÿ™ ÿßÿ™ÿßŸÇ 5√ó4 ŸÖÿ™ÿ±",
            "output": "ŸÖÿ≥ÿßÿ≠ÿ™ = ÿ∑ŸàŸÑ √ó ÿπÿ±ÿ∂ = 5 √ó 4 = 20 ŸÖÿ™ÿ± ŸÖÿ±ÿ®ÿπ"
        },
        {
            "input": "ŸÖÿ≥ÿßÿ≠ÿ™ ÿßÿ™ÿßŸÇ 6√ó3.5 ŸÖÿ™ÿ±ÿü",
            "output": "ŸÖÿ≥ÿßÿ≠ÿ™ = 6 √ó 3.5 = 21 ŸÖÿ™ÿ± ŸÖÿ±ÿ®ÿπ"
        },
        {
            "input": "Calculate area of 8m √ó 5m room",
            "output": "Area = length √ó width = 8 √ó 5 = 40 square meters"
        }
    ]
    
    print("üìö Training Examples:")
    for i, ex in enumerate(examples, 1):
        print(f"\n  Example {i}:")
        print(f"    Input: {ex['input']}")
        print(f"    Output: {ex['output']}")
    
    print("\n" + "-" * 60 + "\n")
    
    # New query
    new_query = "ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÖÿ≥ÿßÿ≠ÿ™ ÿßÿ™ÿßŸÇ 7.5√ó6 ŸÖÿ™ÿ±"
    
    print(f"üéØ New Query: {new_query}")
    print("\n" + "-" * 60 + "\n")
    
    prompt = brain.create_few_shot_prompt(
        task_description="Calculate room area in square meters. Show formula and result.",
        examples=examples,
        current_input=new_query,
        max_examples=3
    )
    
    print("üìù Generated Few-Shot Prompt:")
    print(prompt)


def test_chain_of_thought():
    """Test chain-of-thought reasoning."""
    print_section("3. CHAIN-OF-THOUGHT REASONING")
    
    brain = SuperAIBrain()
    
    # Complex problem
    problem = """
    €å⁄© ÿ≥ÿßÿÆÿ™ŸÖÿßŸÜ 5 ÿ∑ÿ®ŸÇŸá ÿ®ÿß ÿßÿ®ÿπÿßÿØ Ÿáÿ± ÿ∑ÿ®ŸÇŸá 12√ó15 ŸÖÿ™ÿ± ŸÖ€å‚ÄåÿÆŸàÿßŸá€åŸÖ ÿ®ÿ≥ÿßÿ≤€åŸÖ.
    ÿßÿ±ÿ™ŸÅÿßÿπ Ÿáÿ± ÿ∑ÿ®ŸÇŸá 3 ŸÖÿ™ÿ± ÿßÿ≥ÿ™.
    ⁄ÜŸÜÿØ ÿ¢ÿ¨ÿ± Ÿà ⁄ÜŸÜÿØ ÿ™ŸÜ ÿ≥€åŸÖÿßŸÜ ÿ®ÿ±ÿß€å ÿ≥ÿßÿÆÿ™ ÿØ€åŸàÿßÿ±Ÿáÿß€å ÿÆÿßÿ±ÿ¨€å ŸÜ€åÿßÿ≤ ÿØÿßÿ±€åŸÖÿü
    (ÿ∂ÿÆÿßŸÖÿ™ ÿØ€åŸàÿßÿ± ÿÆÿßÿ±ÿ¨€å 30 ÿ≥ÿßŸÜÿ™€å‚ÄåŸÖÿ™ÿ±)
    """
    
    print("üß© Complex Problem:")
    print(problem)
    print("\n" + "-" * 60 + "\n")
    
    prompt = brain.create_chain_of_thought_prompt(
        problem=problem.strip(),
        domain="architectural engineering"
    )
    
    print("üß† Chain-of-Thought Prompt:")
    print(prompt)


def test_cached_system_prompt():
    """Test cached system prompt (Anthropic style)."""
    print_section("4. CACHED SYSTEM PROMPT")
    
    brain = SuperAIBrain()
    
    # Training examples
    training_examples = [
        {
            "input": "ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÖÿ≥ÿßÿ≠ÿ™ ÿßÿ™ÿßŸÇ 5√ó4 ŸÖÿ™ÿ±",
            "output": "ŸÖÿ≥ÿßÿ≠ÿ™ = 5 √ó 4 = 20 ŸÖÿ™ÿ± ŸÖÿ±ÿ®ÿπ"
        },
        {
            "input": "⁄ÜŸÜÿØ ÿ¢ÿ¨ÿ± ÿ®ÿ±ÿß€å ÿØ€åŸàÿßÿ± 10 ŸÖÿ™ÿ±€å ŸÜ€åÿßÿ≤ ÿßÿ≥ÿ™ÿü",
            "output": "ŸÖÿ≥ÿßÿ≠ÿ™ = 10 √ó 3 = 30 m¬≤\nÿ¢ÿ¨ÿ± = 30 √ó 60 = 1,800 ÿπÿØÿØ"
        },
        {
            "input": "ÿ≠ÿØÿßŸÇŸÑ ÿßÿ±ÿ™ŸÅÿßÿπ ÿ≥ŸÇŸÅÿü",
            "output": "ÿ∑ÿ®ŸÇ ŸÖÿ®ÿ≠ÿ´ 19: ÿ≠ÿØÿßŸÇŸÑ 2.4 ŸÖÿ™ÿ± ÿ®ÿ±ÿß€å ÿßÿ™ÿßŸÇ‚ÄåŸáÿß€å ÿßÿµŸÑ€å"
        },
        {
            "input": "Calculate volume of 6√ó4√ó2.8m room",
            "output": "Volume = 6 √ó 4 √ó 2.8 = 67.2 cubic meters"
        },
        {
            "input": "Foundation depth for 3-story building?",
            "output": "Minimum: 1.5-2 meters below ground level, depending on soil conditions"
        }
    ]
    
    print("üìö Creating Cached Prompt with Examples:")
    print(f"  Total examples: {len(training_examples)}")
    print("\n" + "-" * 60 + "\n")
    
    cached = brain.create_cached_system_prompt(
        system_role="KURDO-AI - Expert Architectural Assistant",
        training_examples=training_examples,
        max_examples=5
    )
    
    print("‚úÖ Cached Prompt Created:")
    print(f"  Cache ID: {cached.get('cache_id', 'N/A')}")
    print(f"  Examples cached: {cached.get('num_examples', 0)}")
    print(f"  Estimated tokens: {cached.get('estimated_tokens', 0)}")
    print(f"\n  Usage: {cached.get('usage', 'N/A')}")
    
    print("\n" + "-" * 60 + "\n")
    print("üìÑ Cached Content Preview (first 500 chars):")
    content = cached.get('cached_content', '')
    print(content[:500] + "..." if len(content) > 500 else content)


def test_prompt_statistics():
    """Test prompt statistics."""
    print_section("5. PROMPT ENGINEERING STATISTICS")
    
    brain = SuperAIBrain()
    
    # Use some templates first to generate stats
    brain.use_prompt_template("arch_calculation", task="test", given_values="test", required_output="test")
    brain.use_prompt_template("code_generation", language="Python", task="test", requirements="test")
    
    stats = brain.get_prompt_statistics()
    
    print("üìä Usage Statistics:")
    print_result(stats)


def test_comparison():
    """Test comparison with training methods."""
    print_section("6. PROMPT ENGINEERING vs TRAINING METHODS")
    
    brain = SuperAIBrain()
    
    comparison = brain.compare_prompt_vs_training()
    
    print("‚öñÔ∏è  Detailed Comparison:")
    print_result(comparison)
    
    # Print summary table
    print("\nüìä Quick Comparison:")
    print("-" * 100)
    print(f"{'Method':<25} {'Setup Time':<15} {'Cost':<15} {'GPU Required':<15} {'Best For':<30}")
    print("-" * 100)
    
    methods_data = [
        ("Prompt Engineering", "Instant", "$0", "No", "Prototyping, no data"),
        ("LoRA", "1-3 hours", "$0", "Yes (6GB+)", "Multiple tasks, limited GPU"),
        ("Fine-Tuning", "2-10 hours", "$10-50", "Yes (40GB+)", "Production, best quality")
    ]
    
    for method, time, cost, gpu, best_for in methods_data:
        print(f"{method:<25} {time:<15} {cost:<15} {gpu:<15} {best_for:<30}")
    
    print("-" * 100)


def test_all_three_methods():
    """Test and compare all three training approaches."""
    print_section("7. ALL THREE METHODS: COMPARISON DEMO")
    
    brain = SuperAIBrain()
    
    # Sample architectural data
    sample_data = [
        {"input": "ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÖÿ≥ÿßÿ≠ÿ™ 5√ó4", "output": "20 ŸÖÿ™ÿ± ŸÖÿ±ÿ®ÿπ"},
        {"input": "ÿ≠ÿ¨ŸÖ 6√ó4√ó3", "output": "72 ŸÖÿ™ÿ± ŸÖ⁄©ÿπÿ®"},
        {"input": "ÿ¢ÿ¨ÿ± ÿ®ÿ±ÿß€å 10 ŸÖÿ™ÿ±", "output": "1800 ÿπÿØÿØ"}
    ]
    
    print("üéØ Task: Train/Configure KURDO-AI for architectural calculations")
    print(f"üìä Sample Data: {len(sample_data)} examples")
    print("\n" + "-" * 60 + "\n")
    
    # Method 1: Prompt Engineering (instant)
    print("1Ô∏è‚É£  PROMPT ENGINEERING (Instant, No Training)")
    print("   ‚úÖ Setup: Create few-shot prompt")
    print("   ‚è±Ô∏è  Time: 0 seconds")
    print("   üí∞ Cost: $0")
    print("   üìù Usage: Include examples in every API call")
    
    few_shot = brain.create_few_shot_prompt(
        task_description="Calculate architectural values",
        examples=sample_data,
        current_input="ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ŸÖÿ≥ÿßÿ≠ÿ™ 7√ó5",
        max_examples=3
    )
    print(f"\n   Sample prompt length: {len(few_shot)} characters\n")
    
    # Method 2: LoRA (fast training)
    print("2Ô∏è‚É£  LoRA (Fast Training)")
    print("   ‚úÖ Setup: Train adapter on GPU")
    print("   ‚è±Ô∏è  Time: 1-2 hours (RTX 3060)")
    print("   üí∞ Cost: $0 (local)")
    print("   üìù Usage: Load adapter, then inference")
    print("   üíæ Adapter size: ~50MB\n")
    
    # Method 3: Fine-Tuning (best quality)
    print("3Ô∏è‚É£  FULL FINE-TUNING (Best Quality)")
    print("   ‚úÖ Setup: Full model training")
    print("   ‚è±Ô∏è  Time: 4-8 hours (A100)")
    print("   üí∞ Cost: $0 (local) or $20-50 (cloud)")
    print("   üìù Usage: Use fine-tuned model directly")
    print("   üíæ Model size: ~14GB\n")
    
    print("-" * 60)
    print("\nüìã RECOMMENDATION:")
    print("  ‚Ä¢ Start with Prompt Engineering (instant)")
    print("  ‚Ä¢ Collect real usage data")
    print("  ‚Ä¢ Train LoRA if you have GPU (50-100 examples)")
    print("  ‚Ä¢ Use Fine-Tuning for production (500+ examples)")


def interactive_menu():
    """Interactive test menu."""
    print_section("KURDO-AI PROMPT ENGINEERING - INTERACTIVE DEMO")
    
    menu = """
    Choose a test to run:
    
    1. üìã Prompt Templates (Built-in)
    2. üìö Few-Shot Learning (No Training)
    3. üß† Chain-of-Thought Reasoning
    4. üíæ Cached System Prompt (Anthropic)
    5. üìä Usage Statistics
    6. ‚öñÔ∏è  Comparison: Prompt vs Training
    7. üéØ All Three Methods Demo
    8. üöÄ Run All Tests
    9. ‚ùå Exit
    
    Enter choice (1-9): """
    
    tests = {
        '1': test_prompt_templates,
        '2': test_few_shot_learning,
        '3': test_chain_of_thought,
        '4': test_cached_system_prompt,
        '5': test_prompt_statistics,
        '6': test_comparison,
        '7': test_all_three_methods,
    }
    
    while True:
        try:
            choice = input(menu).strip()
            
            if choice == '9':
                print("\nüëã Goodbye!")
                break
            elif choice == '8':
                print("\nüöÄ Running all tests...\n")
                for test_func in tests.values():
                    try:
                        test_func()
                    except Exception as e:
                        print(f"‚ùå Test failed: {e}\n")
                print("\n‚úÖ All tests complete!")
            elif choice in tests:
                try:
                    tests[choice]()
                except Exception as e:
                    print(f"\n‚ùå Error: {e}\n")
            else:
                print("\n‚ùå Invalid choice. Please enter 1-9.\n")
                
        except KeyboardInterrupt:
            print("\n\nüëã Interrupted by user. Goodbye!")
            break
        except EOFError:
            break


def main():
    """Main entry point."""
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                              ‚ïë
‚ïë          üéØ KURDO-AI PROMPT ENGINEERING - TEST SUITE üéØ                     ‚ïë
‚ïë                                                                              ‚ïë
‚ïë  Training-Free Methods:                                                      ‚ïë
‚ïë    ‚Ä¢ Prompt Templates - Reusable patterns                                   ‚ïë
‚ïë    ‚Ä¢ Few-Shot Learning - Learn from examples                                ‚ïë
‚ïë    ‚Ä¢ Chain-of-Thought - Complex reasoning                                   ‚ïë
‚ïë    ‚Ä¢ Cached Prompts - Cost-effective (Anthropic)                            ‚ïë
‚ïë                                                                              ‚ïë
‚ïë  Advantages:                                                                 ‚ïë
‚ïë    ‚úÖ Instant setup (no training time)                                      ‚ïë
‚ïë    ‚úÖ Zero cost (except inference)                                          ‚ïë
‚ïë    ‚úÖ No GPU required                                                       ‚ïë
‚ïë    ‚úÖ Extremely flexible                                                    ‚ïë
‚ïë                                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    if len(sys.argv) > 1:
        arg = sys.argv[1].lower()
        
        if arg == '--templates':
            test_prompt_templates()
        elif arg == '--few-shot':
            test_few_shot_learning()
        elif arg == '--cot':
            test_chain_of_thought()
        elif arg == '--cached':
            test_cached_system_prompt()
        elif arg == '--stats':
            test_prompt_statistics()
        elif arg == '--compare':
            test_comparison()
        elif arg == '--three-methods':
            test_all_three_methods()
        elif arg == '--all':
            print("\nüöÄ Running all tests...\n")
            test_prompt_templates()
            test_few_shot_learning()
            test_chain_of_thought()
            test_cached_system_prompt()
            test_prompt_statistics()
            test_comparison()
            test_all_three_methods()
            print("\n‚úÖ All tests complete!")
        else:
            print(f"Unknown argument: {arg}")
            print("\nAvailable arguments:")
            print("  --templates      : Test prompt templates")
            print("  --few-shot       : Test few-shot learning")
            print("  --cot            : Test chain-of-thought")
            print("  --cached         : Test cached prompts")
            print("  --stats          : Show statistics")
            print("  --compare        : Compare methods")
            print("  --three-methods  : Demo all three approaches")
            print("  --all            : Run all tests")
            print("  (no args)        : Interactive menu")
    else:
        # Interactive mode
        interactive_menu()


if __name__ == "__main__":
    main()
