{
  "openai": {
    "enabled": true,
    "supported_base_models": [
      "gpt-4o-mini-2024-07-18",
      "gpt-3.5-turbo-0125",
      "gpt-3.5-turbo-1106"
    ],
    "default_hyperparameters": {
      "n_epochs": 3,
      "batch_size": 1,
      "learning_rate_multiplier": 1.0
    }
  },
  "huggingface": {
    "enabled": true,
    "supported_base_models": [
      "google/flan-t5-base",
      "google/flan-t5-small",
      "google/mt5-base",
      "google/mt5-small",
      "facebook/bart-base"
    ],
    "default_training_args": {
      "learning_rate": 2e-5,
      "per_device_train_batch_size": 4,
      "num_train_epochs": 3,
      "weight_decay": 0.01
    }
  },
  "anthropic": {
    "enabled": true,
    "method": "prompt_caching",
    "max_cached_examples": 20,
    "note": "Anthropic doesn't support traditional fine-tuning. We use prompt caching with few-shot examples instead."
  },
  "local_models": {
    "enabled": true,
    "supported_architectures": [
      "seq2seq",
      "causal_lm",
      "masked_lm"
    ],
    "output_directory": "models/fine_tuned"
  },
  "training_data": {
    "formats_supported": [
      "openai_messages",
      "huggingface_json",
      "input_output_pairs"
    ],
    "validation_split": 0.1,
    "max_sequence_length": 512
  }
}
