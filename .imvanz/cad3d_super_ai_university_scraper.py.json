[
  {
    "timestamp": "2025-11-21T23:12:11.382Z",
    "fileName": "cad3d\\super_ai\\university_scraper.py",
    "content": "\"\"\"\r\nUniversity Web Scraper - استخراج محتوا از دانشگاه‌های برتر\r\n\r\nابزارهای scraping برای استخراج خودکار محتوا از منابع دانشگاهی\r\n\"\"\"\r\n\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nimport time\r\nimport logging\r\nfrom typing import Dict, List, Optional\r\nfrom pathlib import Path\r\nimport hashlib\r\nfrom datetime import datetime\r\nimport json\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass UniversityScraper:\r\n    \"\"\"\r\n    Web Scraper برای استخراج محتوا از وب‌سایت‌های دانشگاهی\r\n    \"\"\"\r\n    \r\n    def __init__(self, config: Dict):\r\n        self.config = config\r\n        self.session = requests.Session()\r\n        self.session.headers.update({\r\n            'User-Agent': config['scraping']['user_agent']\r\n        })\r\n        self.timeout = config['scraping']['timeout']\r\n        self.rate_limit = config['scraping']['rate_limit']\r\n        self.retry_attempts = config['scraping']['retry_attempts']\r\n        \r\n        # Cache directory\r\n        self.cache_dir = Path(config['storage']['cache_dir'])\r\n        self.cache_dir.mkdir(exist_ok=True, parents=True)\r\n    \r\n    def fetch_url(self, url: str, retry: int = 0) -> Optional[str]:\r\n        \"\"\"\r\n        دریافت محتوای یک URL با مدیریت خطا و retry\r\n        \r\n        Args:\r\n            url: آدرس URL\r\n            retry: تعداد تلاش مجدد\r\n        \r\n        Returns:\r\n            محتوای HTML یا None\r\n        \"\"\"\r\n        try:\r\n            time.sleep(self.rate_limit)  # Rate limiting\r\n            \r\n            response = self.session.get(url, timeout=self.timeout)\r\n            response.raise_for_status()\r\n            \r\n            logger.info(f\"✓ Fetched: {url}\")\r\n            return response.text\r\n            \r\n        except requests.RequestException as e:\r\n            logger.warning(f\"✗ Error fetching {url}: {e}\")\r\n            \r\n            if retry < self.retry_attempts:\r\n                logger.info(f\"  Retrying ({retry + 1}/{self.retry_attempts})...\")\r\n                return self.fetch_url(url, retry + 1)\r\n            \r\n            return None\r\n    \r\n    def extract_links(self, html: str, base_url: str, filter_pattern: Optional[str] = None) -> List[str]:\r\n        \"\"\"\r\n        استخراج لینک‌ها از HTML\r\n        \r\n        Args:\r\n            html: محتوای HTML\r\n            base_url: URL پایه برای لینک‌های نسبی\r\n            filter_pattern: الگوی فیلتر برای لینک‌ها\r\n        \r\n        Returns:\r\n            لیست لینک‌های استخراج‌شده\r\n        \"\"\"\r\n        soup = BeautifulSoup(html, 'html.parser')\r\n        links = []\r\n        \r\n        for a_tag in soup.find_all('a', href=True):\r\n            href = a_tag['href']\r\n            \r\n            # تبدیل به URL کامل\r\n            if href.startswith('http'):\r\n                full_url = href\r\n            elif href.startswith('/'):\r\n                full_url = base_url.rstrip('/') + href\r\n            else:\r\n                continue\r\n            \r\n            # فیلتر\r\n            if filter_pattern is None or filter_pattern in full_url:\r\n                links.append(full_url)\r\n        \r\n        return list(set(links))  # حذف تکراری\r\n    \r\n    def extract_text_content(self, html: str) -> Dict:\r\n        \"\"\"\r\n        استخراج محتوای متنی از HTML\r\n        \r\n        Returns:\r\n            دیکشنری شامل عنوان، متن، و متادیتا\r\n        \"\"\"\r\n        soup = BeautifulSoup(html, 'html.parser')\r\n        \r\n        # حذف اسکریپت‌ها و استایل‌ها\r\n        for script in soup([\"script\", \"style\"]):\r\n            script.decompose()\r\n        \r\n        # عنوان\r\n        title = soup.find('title')\r\n        title_text = title.get_text().strip() if title else \"No Title\"\r\n        \r\n        # متن اصلی\r\n        text = soup.get_text(separator='\\n', strip=True)\r\n        \r\n        # پاراگراف‌ها\r\n        paragraphs = [p.get_text().strip() for p in soup.find_all('p')]\r\n        \r\n        # هدینگ‌ها\r\n        headings = []\r\n        for i in range(1, 7):\r\n            for h in soup.find_all(f'h{i}'):\r\n                headings.append({\r\n                    'level': i,\r\n                    'text': h.get_text().strip()\r\n                })\r\n        \r\n        return {\r\n            'title': title_text,\r\n            'text': text,\r\n            'paragraphs': paragraphs,\r\n            'headings': headings,\r\n            'length': len(text)\r\n        }\r\n    \r\n    def extract_pdf_links(self, html: str, base_url: str) -> List[str]:\r\n        \"\"\"استخراج لینک‌های PDF از صفحه\"\"\"\r\n        soup = BeautifulSoup(html, 'html.parser')\r\n        pdf_links = []\r\n        \r\n        for a_tag in soup.find_all('a', href=True):\r\n            href = a_tag['href']\r\n            if href.endswith('.pdf') or '.pdf?' in href:\r\n                if href.startswith('http'):\r\n                    pdf_links.append(href)\r\n                elif href.startswith('/'):\r\n                    pdf_links.append(base_url.rstrip('/') + href)\r\n        \r\n        return list(set(pdf_links))\r\n    \r\n    def cache_content(self, url: str, content: Dict) -> None:\r\n        \"\"\"ذخیره محتوا در cache\"\"\"\r\n        # ایجاد نام فایل از URL\r\n        url_hash = hashlib.md5(url.encode()).hexdigest()\r\n        cache_file = self.cache_dir / f\"{url_hash}.json\"\r\n        \r\n        cache_data = {\r\n            'url': url,\r\n            'timestamp': datetime.now().isoformat(),\r\n            'content': content\r\n        }\r\n        \r\n        with open(cache_file, 'w', encoding='utf-8') as f:\r\n            json.dump(cache_data, f, ensure_ascii=False, indent=2)\r\n        \r\n        logger.info(f\"  Cached: {cache_file.name}\")\r\n    \r\n    def get_cached_content(self, url: str) -> Optional[Dict]:\r\n        \"\"\"بازیابی محتوا از cache\"\"\"\r\n        url_hash = hashlib.md5(url.encode()).hexdigest()\r\n        cache_file = self.cache_dir / f\"{url_hash}.json\"\r\n        \r\n        if cache_file.exists():\r\n            with open(cache_file, 'r', encoding='utf-8') as f:\r\n                return json.load(f)\r\n        \r\n        return None\r\n    \r\n    def scrape_page(self, url: str, use_cache: bool = True) -> Optional[Dict]:\r\n        \"\"\"\r\n        استخراج کامل محتوای یک صفحه\r\n        \r\n        Args:\r\n            url: آدرس صفحه\r\n            use_cache: استفاده از cache\r\n        \r\n        Returns:\r\n            دیکشنری شامل تمام محتوای استخراج‌شده\r\n        \"\"\"\r\n        # بررسی cache\r\n        if use_cache:\r\n            cached = self.get_cached_content(url)\r\n            if cached:\r\n                logger.info(f\"  From cache: {url}\")\r\n                return cached['content']\r\n        \r\n        # دریافت HTML\r\n        html = self.fetch_url(url)\r\n        if not html:\r\n            return None\r\n        \r\n        # استخراج محتوا\r\n        content = self.extract_text_content(html)\r\n        \r\n        # استخراج لینک‌ها\r\n        content['links'] = self.extract_links(html, url)\r\n        content['pdf_links'] = self.extract_pdf_links(html, url)\r\n        content['url'] = url\r\n        \r\n        # ذخیره در cache\r\n        self.cache_content(url, content)\r\n        \r\n        return content\r\n\r\n\r\nclass UniversityResourceCollector:\r\n    \"\"\"\r\n    جمع‌آوری منابع از دانشگاه‌های مختلف\r\n    \"\"\"\r\n    \r\n    def __init__(self, universities: Dict, config: Dict):\r\n        self.universities = universities\r\n        self.scraper = UniversityScraper(config)\r\n        self.collected_data = {}\r\n    \r\n    def collect_from_university(\r\n        self,\r\n        university_key: str,\r\n        max_pages: int = 10\r\n    ) -> Dict:\r\n        \"\"\"\r\n        جمع‌آوری محتوا از یک دانشگاه\r\n        \r\n        Args:\r\n            university_key: کلید دانشگاه (مثلا \"MIT\")\r\n            max_pages: حداکثر تعداد صفحات\r\n        \r\n        Returns:\r\n            دیکشنری شامل تمام داده‌های جمع‌آوری‌شده\r\n        \"\"\"\r\n        if university_key not in self.universities:\r\n            logger.error(f\"University not found: {university_key}\")\r\n            return {}\r\n        \r\n        university = self.universities[university_key]\r\n        logger.info(f\"\\n{'='*80}\")\r\n        logger.info(f\"Collecting from: {university['name']}\")\r\n        logger.info(f\"{'='*80}\")\r\n        \r\n        collected = {\r\n            'university': university['name'],\r\n            'resources': {},\r\n            'total_pages': 0,\r\n            'total_pdfs': 0\r\n        }\r\n        \r\n        # جمع‌آوری از هر منبع\r\n        for resource_key, resource_info in university['resources'].items():\r\n            logger.info(f\"\\n  Resource: {resource_key}\")\r\n            logger.info(f\"  URL: {resource_info['url']}\")\r\n            \r\n            # استخراج صفحه اصلی\r\n            content = self.scraper.scrape_page(resource_info['url'])\r\n            \r\n            if content:\r\n                collected['resources'][resource_key] = {\r\n                    'info': resource_info,\r\n                    'main_page': content,\r\n                    'sub_pages': []\r\n                }\r\n                \r\n                collected['total_pages'] += 1\r\n                collected['total_pdfs'] += len(content.get('pdf_links', []))\r\n                \r\n                # استخراج چند صفحه فرعی (محدود)\r\n                sub_links = content.get('links', [])[:max_pages]\r\n                \r\n                for i, link in enumerate(sub_links, 1):\r\n                    if i > max_pages:\r\n                        break\r\n                    \r\n                    sub_content = self.scraper.scrape_page(link)\r\n                    if sub_content:\r\n                        collected['resources'][resource_key]['sub_pages'].append(sub_content)\r\n                        collected['total_pages'] += 1\r\n                        collected['total_pdfs'] += len(sub_content.get('pdf_links', []))\r\n        \r\n        logger.info(f\"\\n  Summary:\")\r\n        logger.info(f\"  Total pages: {collected['total_pages']}\")\r\n        logger.info(f\"  Total PDFs found: {collected['total_pdfs']}\")\r\n        \r\n        self.collected_data[university_key] = collected\r\n        return collected\r\n    \r\n    def collect_from_all(self, max_pages_per_resource: int = 5) -> Dict:\r\n        \"\"\"\r\n        جمع‌آوری از همه دانشگاه‌ها\r\n        \r\n        Args:\r\n            max_pages_per_resource: حداکثر صفحات هر منبع\r\n        \r\n        Returns:\r\n            دیکشنری شامل داده‌های همه دانشگاه‌ها\r\n        \"\"\"\r\n        for university_key in self.universities.keys():\r\n            try:\r\n                self.collect_from_university(university_key, max_pages_per_resource)\r\n            except Exception as e:\r\n                logger.error(f\"Error collecting from {university_key}: {e}\")\r\n        \r\n        return self.collected_data\r\n    \r\n    def get_statistics(self) -> Dict:\r\n        \"\"\"آمار جمع‌آوری\"\"\"\r\n        total_pages = sum(data['total_pages'] for data in self.collected_data.values())\r\n        total_pdfs = sum(data['total_pdfs'] for data in self.collected_data.values())\r\n        \r\n        return {\r\n            'universities_collected': len(self.collected_data),\r\n            'total_pages': total_pages,\r\n            'total_pdfs': total_pdfs,\r\n            'by_university': {\r\n                key: {\r\n                    'pages': data['total_pages'],\r\n                    'pdfs': data['total_pdfs']\r\n                }\r\n                for key, data in self.collected_data.items()\r\n            }\r\n        }\r\n",
    "format": "py"
  },
  {
    "timestamp": "2025-11-21T23:12:11.567Z",
    "fileName": "cad3d\\super_ai\\university_scraper.py",
    "content": "\"\"\"\r\nUniversity Web Scraper - استخراج محتوا از دانشگاه‌های برتر\r\n\r\nابزارهای scraping برای استخراج خودکار محتوا از منابع دانشگاهی\r\n\"\"\"\r\n\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nimport time\r\nimport logging\r\nfrom typing import Dict, List, Optional\r\nfrom pathlib import Path\r\nimport hashlib\r\nfrom datetime import datetime\r\nimport json\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass UniversityScraper:\r\n    \"\"\"\r\n    Web Scraper برای استخراج محتوا از وب‌سایت‌های دانشگاهی\r\n    \"\"\"\r\n    \r\n    def __init__(self, config: Dict):\r\n        self.config = config\r\n        self.session = requests.Session()\r\n        self.session.headers.update({\r\n            'User-Agent': config['scraping']['user_agent']\r\n        })\r\n        self.timeout = config['scraping']['timeout']\r\n        self.rate_limit = config['scraping']['rate_limit']\r\n        self.retry_attempts = config['scraping']['retry_attempts']\r\n        \r\n        # Cache directory\r\n        self.cache_dir = Path(config['storage']['cache_dir'])\r\n        self.cache_dir.mkdir(exist_ok=True, parents=True)\r\n    \r\n    def fetch_url(self, url: str, retry: int = 0) -> Optional[str]:\r\n        \"\"\"\r\n        دریافت محتوای یک URL با مدیریت خطا و retry\r\n        \r\n        Args:\r\n            url: آدرس URL\r\n            retry: تعداد تلاش مجدد\r\n        \r\n        Returns:\r\n            محتوای HTML یا None\r\n        \"\"\"\r\n        try:\r\n            time.sleep(self.rate_limit)  # Rate limiting\r\n            \r\n            response = self.session.get(url, timeout=self.timeout)\r\n            response.raise_for_status()\r\n            \r\n            logger.info(f\"✓ Fetched: {url}\")\r\n            return response.text\r\n            \r\n        except requests.RequestException as e:\r\n            logger.warning(f\"✗ Error fetching {url}: {e}\")\r\n            \r\n            if retry < self.retry_attempts:\r\n                logger.info(f\"  Retrying ({retry + 1}/{self.retry_attempts})...\")\r\n                return self.fetch_url(url, retry + 1)\r\n            \r\n            return None\r\n    \r\n    def extract_links(self, html: str, base_url: str, filter_pattern: Optional[str] = None) -> List[str]:\r\n        \"\"\"\r\n        استخراج لینک‌ها از HTML\r\n        \r\n        Args:\r\n            html: محتوای HTML\r\n            base_url: URL پایه برای لینک‌های نسبی\r\n            filter_pattern: الگوی فیلتر برای لینک‌ها\r\n        \r\n        Returns:\r\n            لیست لینک‌های استخراج‌شده\r\n        \"\"\"\r\n        soup = BeautifulSoup(html, 'html.parser')\r\n        links = []\r\n        \r\n        for a_tag in soup.find_all('a', href=True):\r\n            href = a_tag['href']\r\n            \r\n            # تبدیل به URL کامل\r\n            if href.startswith('http'):\r\n                full_url = href\r\n            elif href.startswith('/'):\r\n                full_url = base_url.rstrip('/') + href\r\n            else:\r\n                continue\r\n            \r\n            # فیلتر\r\n            if filter_pattern is None or filter_pattern in full_url:\r\n                links.append(full_url)\r\n        \r\n        return list(set(links))  # حذف تکراری\r\n    \r\n    def extract_text_content(self, html: str) -> Dict:\r\n        \"\"\"\r\n        استخراج محتوای متنی از HTML\r\n        \r\n        Returns:\r\n            دیکشنری شامل عنوان، متن، و متادیتا\r\n        \"\"\"\r\n        soup = BeautifulSoup(html, 'html.parser')\r\n        \r\n        # حذف اسکریپت‌ها و استایل‌ها\r\n        for script in soup([\"script\", \"style\"]):\r\n            script.decompose()\r\n        \r\n        # عنوان\r\n        title = soup.find('title')\r\n        title_text = title.get_text().strip() if title else \"No Title\"\r\n        \r\n        # متن اصلی\r\n        text = soup.get_text(separator='\\n', strip=True)\r\n        \r\n        # پاراگراف‌ها\r\n        paragraphs = [p.get_text().strip() for p in soup.find_all('p')]\r\n        \r\n        # هدینگ‌ها\r\n        headings = []\r\n        for i in range(1, 7):\r\n            for h in soup.find_all(f'h{i}'):\r\n                headings.append({\r\n                    'level': i,\r\n                    'text': h.get_text().strip()\r\n                })\r\n        \r\n        return {\r\n            'title': title_text,\r\n            'text': text,\r\n            'paragraphs': paragraphs,\r\n            'headings': headings,\r\n            'length': len(text)\r\n        }\r\n    \r\n    def extract_pdf_links(self, html: str, base_url: str) -> List[str]:\r\n        \"\"\"استخراج لینک‌های PDF از صفحه\"\"\"\r\n        soup = BeautifulSoup(html, 'html.parser')\r\n        pdf_links = []\r\n        \r\n        for a_tag in soup.find_all('a', href=True):\r\n            href = a_tag['href']\r\n            if href.endswith('.pdf') or '.pdf?' in href:\r\n                if href.startswith('http'):\r\n                    pdf_links.append(href)\r\n                elif href.startswith('/'):\r\n                    pdf_links.append(base_url.rstrip('/') + href)\r\n        \r\n        return list(set(pdf_links))\r\n    \r\n    def cache_content(self, url: str, content: Dict) -> None:\r\n        \"\"\"ذخیره محتوا در cache\"\"\"\r\n        # ایجاد نام فایل از URL\r\n        url_hash = hashlib.md5(url.encode()).hexdigest()\r\n        cache_file = self.cache_dir / f\"{url_hash}.json\"\r\n        \r\n        cache_data = {\r\n            'url': url,\r\n            'timestamp': datetime.now().isoformat(),\r\n            'content': content\r\n        }\r\n        \r\n        with open(cache_file, 'w', encoding='utf-8') as f:\r\n            json.dump(cache_data, f, ensure_ascii=False, indent=2)\r\n        \r\n        logger.info(f\"  Cached: {cache_file.name}\")\r\n    \r\n    def get_cached_content(self, url: str) -> Optional[Dict]:\r\n        \"\"\"بازیابی محتوا از cache\"\"\"\r\n        url_hash = hashlib.md5(url.encode()).hexdigest()\r\n        cache_file = self.cache_dir / f\"{url_hash}.json\"\r\n        \r\n        if cache_file.exists():\r\n            with open(cache_file, 'r', encoding='utf-8') as f:\r\n                return json.load(f)\r\n        \r\n        return None\r\n    \r\n    def scrape_page(self, url: str, use_cache: bool = True) -> Optional[Dict]:\r\n        \"\"\"\r\n        استخراج کامل محتوای یک صفحه\r\n        \r\n        Args:\r\n            url: آدرس صفحه\r\n            use_cache: استفاده از cache\r\n        \r\n        Returns:\r\n            دیکشنری شامل تمام محتوای استخراج‌شده\r\n        \"\"\"\r\n        # بررسی cache\r\n        if use_cache:\r\n            cached = self.get_cached_content(url)\r\n            if cached:\r\n                logger.info(f\"  From cache: {url}\")\r\n                return cached['content']\r\n        \r\n        # دریافت HTML\r\n        html = self.fetch_url(url)\r\n        if not html:\r\n            return None\r\n        \r\n        # استخراج محتوا\r\n        content = self.extract_text_content(html)\r\n        \r\n        # استخراج لینک‌ها\r\n        content['links'] = self.extract_links(html, url)\r\n        content['pdf_links'] = self.extract_pdf_links(html, url)\r\n        content['url'] = url\r\n        \r\n        # ذخیره در cache\r\n        self.cache_content(url, content)\r\n        \r\n        return content\r\n\r\n\r\nclass UniversityResourceCollector:\r\n    \"\"\"\r\n    جمع‌آوری منابع از دانشگاه‌های مختلف\r\n    \"\"\"\r\n    \r\n    def __init__(self, universities: Dict, config: Dict):\r\n        self.universities = universities\r\n        self.scraper = UniversityScraper(config)\r\n        self.collected_data = {}\r\n    \r\n    def collect_from_university(\r\n        self,\r\n        university_key: str,\r\n        max_pages: int = 10\r\n    ) -> Dict:\r\n        \"\"\"\r\n        جمع‌آوری محتوا از یک دانشگاه\r\n        \r\n        Args:\r\n            university_key: کلید دانشگاه (مثلا \"MIT\")\r\n            max_pages: حداکثر تعداد صفحات\r\n        \r\n        Returns:\r\n            دیکشنری شامل تمام داده‌های جمع‌آوری‌شده\r\n        \"\"\"\r\n        if university_key not in self.universities:\r\n            logger.error(f\"University not found: {university_key}\")\r\n            return {}\r\n        \r\n        university = self.universities[university_key]\r\n        logger.info(f\"\\n{'='*80}\")\r\n        logger.info(f\"Collecting from: {university['name']}\")\r\n        logger.info(f\"{'='*80}\")\r\n        \r\n        collected = {\r\n            'university': university['name'],\r\n            'resources': {},\r\n            'total_pages': 0,\r\n            'total_pdfs': 0\r\n        }\r\n        \r\n        # جمع‌آوری از هر منبع\r\n        for resource_key, resource_info in university['resources'].items():\r\n            logger.info(f\"\\n  Resource: {resource_key}\")\r\n            logger.info(f\"  URL: {resource_info['url']}\")\r\n            \r\n            # استخراج صفحه اصلی\r\n            content = self.scraper.scrape_page(resource_info['url'])\r\n            \r\n            if content:\r\n                collected['resources'][resource_key] = {\r\n                    'info': resource_info,\r\n                    'main_page': content,\r\n                    'sub_pages': []\r\n                }\r\n                \r\n                collected['total_pages'] += 1\r\n                collected['total_pdfs'] += len(content.get('pdf_links', []))\r\n                \r\n                # استخراج چند صفحه فرعی (محدود)\r\n                sub_links = content.get('links', [])[:max_pages]\r\n                \r\n                for i, link in enumerate(sub_links, 1):\r\n                    if i > max_pages:\r\n                        break\r\n                    \r\n                    sub_content = self.scraper.scrape_page(link)\r\n                    if sub_content:\r\n                        collected['resources'][resource_key]['sub_pages'].append(sub_content)\r\n                        collected['total_pages'] += 1\r\n                        collected['total_pdfs'] += len(sub_content.get('pdf_links', []))\r\n        \r\n        logger.info(f\"\\n  Summary:\")\r\n        logger.info(f\"  Total pages: {collected['total_pages']}\")\r\n        logger.info(f\"  Total PDFs found: {collected['total_pdfs']}\")\r\n        \r\n        self.collected_data[university_key] = collected\r\n        return collected\r\n    \r\n    def collect_from_all(self, max_pages_per_resource: int = 5) -> Dict:\r\n        \"\"\"\r\n        جمع‌آوری از همه دانشگاه‌ها\r\n        \r\n        Args:\r\n            max_pages_per_resource: حداکثر صفحات هر منبع\r\n        \r\n        Returns:\r\n            دیکشنری شامل داده‌های همه دانشگاه‌ها\r\n        \"\"\"\r\n        for university_key in self.universities.keys():\r\n            try:\r\n                self.collect_from_university(university_key, max_pages_per_resource)\r\n            except Exception as e:\r\n                logger.error(f\"Error collecting from {university_key}: {e}\")\r\n        \r\n        return self.collected_data\r\n    \r\n    def get_statistics(self) -> Dict:\r\n        \"\"\"آمار جمع‌آوری\"\"\"\r\n        total_pages = sum(data['total_pages'] for data in self.collected_data.values())\r\n        total_pdfs = sum(data['total_pdfs'] for data in self.collected_data.values())\r\n        \r\n        return {\r\n            'universities_collected': len(self.collected_data),\r\n            'total_pages': total_pages,\r\n            'total_pdfs': total_pdfs,\r\n            'by_university': {\r\n                key: {\r\n                    'pages': data['total_pages'],\r\n                    'pdfs': data['total_pdfs']\r\n                }\r\n                for key, data in self.collected_data.items()\r\n            }\r\n        }\r\n",
    "format": "py"
  }
]