[
  {
    "timestamp": "2025-11-21T22:42:12.422Z",
    "fileName": "cad3d\\super_ai\\peft_system.py",
    "content": "\"\"\"\r\nPEFT System Manager\r\n\r\nProvides a lightweight manager around Parameter-Efficient Fine-Tuning techniques\r\n(e.g., Prefix-Tuning, P-Tuning, IA3, (Q)LoRA) to complement existing methods.\r\n\r\nThis module is designed to work even without the external 'peft' package.\r\nIf 'peft' is available, it will expose additional capability flags in status.\r\n\"\"\"\r\nfrom __future__ import annotations\r\n\r\nfrom dataclasses import dataclass, asdict\r\nfrom typing import Dict, List, Optional, Any\r\nfrom datetime import datetime\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\n@dataclass\r\nclass AdapterInfo:\r\n    name: str\r\n    technique: str  # e.g., 'prefix_tuning', 'p_tuning', 'ia3', 'qlora', 'adalora', 'lora'\r\n    path: Optional[str] = None\r\n    config: Optional[Dict[str, Any]] = None\r\n    created_at: str = datetime.now().isoformat()\r\n    loaded: bool = False\r\n\r\n\r\nclass PEFTManager:\r\n    def __init__(self, device: str = \"auto\"):\r\n        self.device = device\r\n        self.techniques: List[str] = [\r\n            \"prefix_tuning\",\r\n            \"p_tuning\",\r\n            \"ia3\",\r\n            \"adalora\",\r\n            \"qlora\",\r\n            \"lora\"\r\n        ]\r\n        self.adapters: Dict[str, AdapterInfo] = {}\r\n        self.active_adapter: Optional[str] = None\r\n\r\n        # Optional dependency detection\r\n        try:\r\n            import peft  # type: ignore\r\n            self._has_peft = True\r\n            self._peft_version = getattr(peft, \"__version__\", \"unknown\")\r\n            logger.info(f\"PEFT library detected: v{self._peft_version}\")\r\n        except Exception:\r\n            self._has_peft = False\r\n            self._peft_version = None\r\n            logger.info(\"PEFT library not installed; running in lightweight mode\")\r\n\r\n        # Register a default demo adapter\r\n        self.register_adapter(\r\n            name=\"architectural_prefix\",\r\n            technique=\"prefix_tuning\",\r\n            config={\"virtual_tokens\": 16, \"domain\": \"architecture\"}\r\n        )\r\n\r\n    def list_techniques(self) -> List[str]:\r\n        return list(self.techniques)\r\n\r\n    def register_adapter(self, name: str, technique: str, path: Optional[str] = None, config: Optional[Dict[str, Any]] = None) -> AdapterInfo:\r\n        if technique not in self.techniques:\r\n            raise ValueError(f\"Unknown PEFT technique: {technique}\")\r\n        info = AdapterInfo(name=name, technique=technique, path=path, config=config or {})\r\n        self.adapters[name] = info\r\n        logger.info(f\"Registered PEFT adapter '{name}' with technique '{technique}'\")\r\n        return info\r\n\r\n    def load_adapter(self, name: str) -> Dict[str, Any]:\r\n        if name not in self.adapters:\r\n            return {\"status\": \"error\", \"message\": f\"Adapter not found: {name}\"}\r\n        # Simulate load\r\n        for a in self.adapters.values():\r\n            a.loaded = False\r\n        self.adapters[name].loaded = True\r\n        self.active_adapter = name\r\n        return {\"status\": \"success\", \"active_adapter\": name}\r\n\r\n    def unload_adapter(self) -> Dict[str, Any]:\r\n        if self.active_adapter and self.active_adapter in self.adapters:\r\n            self.adapters[self.active_adapter].loaded = False\r\n        self.active_adapter = None\r\n        return {\"status\": \"success\", \"active_adapter\": None}\r\n\r\n    def apply(self, query: str, task_type: Optional[str] = None, adapter: Optional[str] = None, technique: Optional[str] = None, **kwargs) -> Dict[str, Any]:\r\n        # Determine adapter\r\n        adapter_name = adapter or self.active_adapter or next(iter(self.adapters), None)\r\n        if not adapter_name:\r\n            return {\"status\": \"error\", \"message\": \"No adapters registered\"}\r\n        if adapter_name not in self.adapters:\r\n            return {\"status\": \"error\", \"message\": f\"Adapter not found: {adapter_name}\"}\r\n\r\n        # Load if needed\r\n        if self.active_adapter != adapter_name:\r\n            self.load_adapter(adapter_name)\r\n\r\n        info = self.adapters[adapter_name]\r\n        used_tech = technique or info.technique\r\n\r\n        # Simulated application — in real integration, we'd wrap base model forward with PEFT modules\r\n        note = (\r\n            f\"Applied PEFT ({used_tech}) adapter '{adapter_name}'\"\r\n            + (f\" on task '{task_type}'\" if task_type else \"\")\r\n        )\r\n\r\n        return {\r\n            \"status\": \"success\",\r\n            \"technique\": used_tech,\r\n            \"adapter\": adapter_name,\r\n            \"note\": note,\r\n            \"peft_available\": self._has_peft,\r\n            \"peft_version\": self._peft_version\r\n        }\r\n\r\n    def get_status(self) -> Dict[str, Any]:\r\n        return {\r\n            \"peft_available\": self._has_peft,\r\n            \"peft_version\": self._peft_version,\r\n            \"device\": self.device,\r\n            \"active_adapter\": self.active_adapter,\r\n            \"adapters\": [asdict(a) for a in self.adapters.values()],\r\n            \"techniques\": self.list_techniques()\r\n        }\r\n\r\n    def compare_with_others(self) -> Dict[str, Any]:\r\n        return {\r\n            \"peft\": {\r\n                \"type\": \"Parameter-Efficient Fine-Tuning\",\r\n                \"setup_time\": \"Minutes to Hours\",\r\n                \"cost\": \"Low ($0-$50)\",\r\n                \"gpu_required\": False,\r\n                \"quality\": \"Very Good\",\r\n                \"flexibility\": \"High (multiple techniques)\",\r\n                \"best_for\": [\r\n                    \"Adapting base models quickly\",\r\n                    \"Limited compute environments\",\r\n                    \"Multiple domain adapters\",\r\n                    \"Updating behavior without full retrain\"\r\n                ],\r\n                \"techniques\": self.list_techniques()\r\n            }\r\n        }\r\n",
    "format": "py"
  },
  {
    "timestamp": "2025-11-21T22:42:33.507Z",
    "fileName": "cad3d\\super_ai\\peft_system.py",
    "content": "\"\"\"\r\nPEFT System Manager\r\n\r\nProvides a lightweight manager around Parameter-Efficient Fine-Tuning techniques\r\n(e.g., Prefix-Tuning, P-Tuning, IA3, (Q)LoRA) to complement existing methods.\r\n\r\nThis module is designed to work even without the external 'peft' package.\r\nIf 'peft' is available, it will expose additional capability flags in status.\r\n\"\"\"\r\nfrom __future__ import annotations\r\n\r\nfrom dataclasses import dataclass, asdict\r\nfrom typing import Dict, List, Optional, Any\r\nfrom datetime import datetime\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\n@dataclass\r\nclass AdapterInfo:\r\n    name: str\r\n    technique: str  # e.g., 'prefix_tuning', 'p_tuning', 'ia3', 'qlora', 'adalora', 'lora'\r\n    path: Optional[str] = None\r\n    config: Optional[Dict[str, Any]] = None\r\n    created_at: str = datetime.now().isoformat()\r\n    loaded: bool = False\r\n\r\n\r\nclass PEFTManager:\r\n    def __init__(self, device: str = \"auto\"):\r\n        self.device = device\r\n        self.techniques: List[str] = [\r\n            \"prefix_tuning\",\r\n            \"p_tuning\",\r\n            \"ia3\",\r\n            \"adalora\",\r\n            \"qlora\",\r\n            \"lora\"\r\n        ]\r\n        self.adapters: Dict[str, AdapterInfo] = {}\r\n        self.active_adapter: Optional[str] = None\r\n\r\n        # Optional dependency detection\r\n        try:\r\n            import peft  # type: ignore\r\n            self._has_peft = True\r\n            self._peft_version = getattr(peft, \"__version__\", \"unknown\")\r\n            logger.info(f\"PEFT library detected: v{self._peft_version}\")\r\n        except Exception:\r\n            self._has_peft = False\r\n            self._peft_version = None\r\n            logger.info(\"PEFT library not installed; running in lightweight mode\")\r\n\r\n        # Register a default demo adapter\r\n        self.register_adapter(\r\n            name=\"architectural_prefix\",\r\n            technique=\"prefix_tuning\",\r\n            config={\"virtual_tokens\": 16, \"domain\": \"architecture\"}\r\n        )\r\n\r\n    def list_techniques(self) -> List[str]:\r\n        return list(self.techniques)\r\n\r\n    def register_adapter(self, name: str, technique: str, path: Optional[str] = None, config: Optional[Dict[str, Any]] = None) -> AdapterInfo:\r\n        if technique not in self.techniques:\r\n            raise ValueError(f\"Unknown PEFT technique: {technique}\")\r\n        info = AdapterInfo(name=name, technique=technique, path=path, config=config or {})\r\n        self.adapters[name] = info\r\n        logger.info(f\"Registered PEFT adapter '{name}' with technique '{technique}'\")\r\n        return info\r\n\r\n    def load_adapter(self, name: str) -> Dict[str, Any]:\r\n        if name not in self.adapters:\r\n            return {\"status\": \"error\", \"message\": f\"Adapter not found: {name}\"}\r\n        # Simulate load\r\n        for a in self.adapters.values():\r\n            a.loaded = False\r\n        self.adapters[name].loaded = True\r\n        self.active_adapter = name\r\n        return {\"status\": \"success\", \"active_adapter\": name}\r\n\r\n    def unload_adapter(self) -> Dict[str, Any]:\r\n        if self.active_adapter and self.active_adapter in self.adapters:\r\n            self.adapters[self.active_adapter].loaded = False\r\n        self.active_adapter = None\r\n        return {\"status\": \"success\", \"active_adapter\": None}\r\n\r\n    def apply(self, query: str, task_type: Optional[str] = None, adapter: Optional[str] = None, technique: Optional[str] = None, **kwargs) -> Dict[str, Any]:\r\n        # Determine adapter\r\n        adapter_name = adapter or self.active_adapter or next(iter(self.adapters), None)\r\n        if not adapter_name:\r\n            return {\"status\": \"error\", \"message\": \"No adapters registered\"}\r\n        if adapter_name not in self.adapters:\r\n            return {\"status\": \"error\", \"message\": f\"Adapter not found: {adapter_name}\"}\r\n\r\n        # Load if needed\r\n        if self.active_adapter != adapter_name:\r\n            self.load_adapter(adapter_name)\r\n\r\n        info = self.adapters[adapter_name]\r\n        used_tech = technique or info.technique\r\n\r\n        # Simulated application — in real integration, we'd wrap base model forward with PEFT modules\r\n        note = (\r\n            f\"Applied PEFT ({used_tech}) adapter '{adapter_name}'\"\r\n            + (f\" on task '{task_type}'\" if task_type else \"\")\r\n        )\r\n\r\n        return {\r\n            \"status\": \"success\",\r\n            \"technique\": used_tech,\r\n            \"adapter\": adapter_name,\r\n            \"note\": note,\r\n            \"peft_available\": self._has_peft,\r\n            \"peft_version\": self._peft_version\r\n        }\r\n\r\n    def get_status(self) -> Dict[str, Any]:\r\n        return {\r\n            \"peft_available\": self._has_peft,\r\n            \"peft_version\": self._peft_version,\r\n            \"device\": self.device,\r\n            \"active_adapter\": self.active_adapter,\r\n            \"adapters\": [asdict(a) for a in self.adapters.values()],\r\n            \"techniques\": self.list_techniques()\r\n        }\r\n\r\n    def compare_with_others(self) -> Dict[str, Any]:\r\n        return {\r\n            \"peft\": {\r\n                \"type\": \"Parameter-Efficient Fine-Tuning\",\r\n                \"setup_time\": \"Minutes to Hours\",\r\n                \"cost\": \"Low ($0-$50)\",\r\n                \"gpu_required\": False,\r\n                \"quality\": \"Very Good\",\r\n                \"flexibility\": \"High (multiple techniques)\",\r\n                \"best_for\": [\r\n                    \"Adapting base models quickly\",\r\n                    \"Limited compute environments\",\r\n                    \"Multiple domain adapters\",\r\n                    \"Updating behavior without full retrain\"\r\n                ],\r\n                \"techniques\": self.list_techniques()\r\n            }\r\n        }\r\n",
    "format": "py"
  }
]