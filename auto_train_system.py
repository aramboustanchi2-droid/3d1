"""
Automatic Learning System
Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±

Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ… Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø±:
1. Ø¯ÛŒØªØ§Ø³Øª Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯
2. Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
3. Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
4. Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
"""

import os
import time
from pathlib import Path
from typing import Dict, List, Optional
import json

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch not installed")

import numpy as np
import cv2


class AutoTrainingSystem:
    """
    Ø³ÛŒØ³ØªÙ… Ø¢Ù…ÙˆØ²Ø´ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª:
    - ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ DXF/Image Ù…ÙˆØ¬ÙˆØ¯
    - ØªÙˆÙ„ÛŒØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± annotation
    - Ø¢Ù…ÙˆØ²Ø´ Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ
    - Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
    """
    
    def __init__(
        self,
        data_dir: str = "training_data",
        models_dir: str = "trained_models",
        device: str = "auto"
    ):
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorch required")
        
        self.data_dir = Path(data_dir)
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(exist_ok=True, parents=True)
        
        # Setup device
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        self.training_stats = {
            'total_samples': 0,
            'epochs_completed': 0,
            'best_loss': float('inf'),
            'training_time': 0
        }
        
        print("ğŸ¤– Automatic Training System Initialized")
        print(f"   Device: {self.device}")
        print(f"   Data dir: {self.data_dir}")
        print(f"   Models dir: {self.models_dir}")
    
    def collect_training_data(self) -> int:
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
        print("\nğŸ“¦ Collecting training data...")
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ
        image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            image_files.extend(self.data_dir.glob(ext))
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ DXF
        dxf_files = list(self.data_dir.glob('*.dxf'))
        
        total = len(image_files) + len(dxf_files)
        
        print(f"   Found {len(image_files)} images")
        print(f"   Found {len(dxf_files)} DXF files")
        print(f"   Total: {total} files")
        
        self.training_stats['total_samples'] = total
        
        return total
    
    def auto_generate_annotations(self, image_path: str) -> Dict:
        """ØªÙˆÙ„ÛŒØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± annotation Ø§Ø² ØªØµÙˆÛŒØ±"""
        try:
            from cad3d.neural_cad_detector import NeuralCADDetector, DetectorConfig
            
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Neural detector Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ annotation Ø§ÙˆÙ„ÛŒÙ‡
            config = DetectorConfig(quality="high")
            detector = NeuralCADDetector(device="auto", config=config)
            
            result = detector.vectorize_drawing(image_path, scale_mm_per_pixel=1.0)
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ annotation format
            annotation = {
                'num_polygons': len(result.polygons),
                'num_lines': len(result.lines),
                'confidence': 0.8,  # Initial confidence
                'auto_generated': True
            }
            
            return annotation
            
        except Exception as e:
            print(f"   Warning: Auto-annotation failed for {image_path}: {e}")
            return {}
    
    def train_vision_transformer(
        self,
        num_epochs: int = 20,
        batch_size: int = 4,
        learning_rate: float = 1e-4
    ) -> Dict:
        """Ø¢Ù…ÙˆØ²Ø´ Vision Transformer"""
        print("\nğŸ“ Training Vision Transformer...")
        print(f"   Epochs: {num_epochs}")
        print(f"   Batch size: {batch_size}")
        print(f"   Learning rate: {learning_rate}")
        
        try:
            from cad3d.vision_transformer_cad import (
                VisionTransformerCAD,
                VisionTransformerConfig
            )
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
            config = VisionTransformerConfig(
                image_size=256,
                patch_size=16,
                num_classes=50,
                dim=512,
                depth=8,
                heads=8,
                mlp_dim=2048,
                dropout=0.1,
                predict_depth=True,
                predict_height=True,
                predict_material=True
            )
            
            model = VisionTransformerCAD(config).to(self.device)
            optimizer = optim.AdamW(
                model.parameters(),
                lr=learning_rate,
                weight_decay=0.01
            )
            
            # Learning rate scheduler
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=num_epochs
            )
            
            history = []
            best_loss = float('inf')
            
            start_time = time.time()
            
            print("\n" + "="*70)
            print("Training Progress")
            print("="*70)
            
            for epoch in range(num_epochs):
                model.train()
                epoch_losses = []
                
                # ØªÙˆÙ„ÛŒØ¯ batch Ù‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ (Ø¯Ø± production Ø§Ø² DataLoader ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯)
                num_batches = 10
                
                for batch_idx in range(num_batches):
                    # Ø¯ÛŒØªØ§ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ
                    images = torch.randn(batch_size, 3, 256, 256).to(self.device)
                    
                    # Targets ØªØµØ§Ø¯ÙÛŒ
                    semantic_target = torch.randint(0, 50, (batch_size, 1024)).to(self.device)
                    height_target = torch.rand(batch_size, 1024, 1).to(self.device) * 5000  # max 5m
                    depth_target = torch.rand(batch_size, 1024, 1).to(self.device)
                    material_target = torch.randint(0, 10, (batch_size, 1024)).to(self.device)
                    
                    # Forward
                    optimizer.zero_grad()
                    outputs = model(images)
                    
                    # Multi-task loss
                    loss_semantic = nn.CrossEntropyLoss()(
                        outputs['semantic'].view(-1, 50),
                        semantic_target.view(-1)
                    )
                    
                    loss_height = nn.MSELoss()(
                        outputs['height'],
                        height_target
                    )
                    
                    loss_depth = nn.MSELoss()(
                        outputs['depth'],
                        depth_target
                    )
                    
                    loss_material = nn.CrossEntropyLoss()(
                        outputs['material'].view(-1, 10),
                        material_target.view(-1)
                    )
                    
                    # Combined loss
                    total_loss = (
                        1.0 * loss_semantic +
                        0.5 * loss_height +
                        0.5 * loss_depth +
                        0.3 * loss_material
                    )
                    
                    # Backward
                    total_loss.backward()
                    
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    
                    epoch_losses.append(total_loss.item())
                
                # Epoch stats
                avg_loss = np.mean(epoch_losses)
                history.append({
                    'epoch': epoch + 1,
                    'loss': avg_loss,
                    'lr': scheduler.get_last_lr()[0]
                })
                
                # Update best model
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
                    model_path = self.models_dir / "vit_best.pth"
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'loss': avg_loss,
                        'config': config.__dict__
                    }, model_path)
                
                # Print progress
                print(f"Epoch {epoch+1:3d}/{num_epochs} | "
                      f"Loss: {avg_loss:.4f} | "
                      f"Best: {best_loss:.4f} | "
                      f"LR: {scheduler.get_last_lr()[0]:.6f}")
                
                scheduler.step()
            
            training_time = time.time() - start_time
            
            print("="*70)
            print(f"âœ… Training complete!")
            print(f"   Best loss: {best_loss:.4f}")
            print(f"   Training time: {training_time:.2f}s")
            print(f"   Model saved to: {self.models_dir / 'vit_best.pth'}")
            
            self.training_stats['epochs_completed'] = num_epochs
            self.training_stats['best_loss'] = best_loss
            self.training_stats['training_time'] = training_time
            
            return {
                'success': True,
                'best_loss': best_loss,
                'history': history,
                'training_time': training_time
            }
            
        except Exception as e:
            print(f"âŒ Training failed: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}
    
    def evaluate_model(self, model_path: str) -> Dict:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡"""
        print(f"\nğŸ“Š Evaluating model: {model_path}")
        
        try:
            from cad3d.vision_transformer_cad import (
                VisionTransformerCAD,
                VisionTransformerConfig
            )
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ checkpoint
            checkpoint = torch.load(model_path, map_location=self.device)
            
            config = VisionTransformerConfig(**checkpoint['config'])
            model = VisionTransformerCAD(config).to(self.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
            
            print(f"   Loaded from epoch {checkpoint['epoch']}")
            print(f"   Training loss: {checkpoint['loss']:.4f}")
            
            # ØªØ³Øª inference
            test_images = torch.randn(5, 3, 256, 256).to(self.device)
            
            times = []
            with torch.no_grad():
                for img in test_images:
                    start = time.time()
                    _ = model(img.unsqueeze(0))
                    if self.device.type == 'cuda':
                        torch.cuda.synchronize()
                    times.append(time.time() - start)
            
            avg_time = np.mean(times)
            fps = 1.0 / avg_time
            
            print(f"   Inference time: {avg_time*1000:.2f} ms")
            print(f"   FPS: {fps:.2f}")
            
            return {
                'avg_inference_time_ms': avg_time * 1000,
                'fps': fps,
                'training_loss': checkpoint['loss']
            }
            
        except Exception as e:
            print(f"âŒ Evaluation failed: {e}")
            return {}
    
    def save_training_report(self, filename: str = "training_report.json"):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø¢Ù…ÙˆØ²Ø´"""
        report_path = self.models_dir / filename
        
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'device': str(self.device),
            'statistics': self.training_stats,
            'system_info': {
                'torch_version': torch.__version__ if TORCH_AVAILABLE else None,
                'cuda_available': torch.cuda.is_available() if TORCH_AVAILABLE else False
            }
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ Training report saved to: {report_path}")
        
        return report


def main():
    """Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ø¢Ù…ÙˆØ²Ø´ Ø®ÙˆØ¯Ú©Ø§Ø±"""
    print("="*70)
    print("ğŸ¤– Automatic Learning System")
    print("="*70)
    
    if not TORCH_AVAILABLE:
        print("âŒ PyTorch not available")
        print("Install: pip install torch torchvision")
        return
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø³ÛŒØ³ØªÙ…
    system = AutoTrainingSystem(
        data_dir="training_data",
        models_dir="trained_models",
        device="auto"
    )
    
    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡
    num_samples = system.collect_training_data()
    
    if num_samples == 0:
        print("\nâš ï¸ No training data found!")
        print("   Create 'training_data' folder and add images/DXF files")
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
        print("\n   Creating sample training data...")
        data_dir = Path("training_data")
        data_dir.mkdir(exist_ok=True)
        
        for i in range(5):
            img = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
            cv2.imwrite(str(data_dir / f"sample_{i}.png"), img)
        
        print(f"   Created 5 sample images in {data_dir}")
        num_samples = 5
    
    # Ø¢Ù…ÙˆØ²Ø´ Vision Transformer
    print("\n" + "ğŸ“ Starting Training".center(70, "="))
    
    result = system.train_vision_transformer(
        num_epochs=10,
        batch_size=2,
        learning_rate=1e-4
    )
    
    if result['success']:
        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
        model_path = system.models_dir / "vit_best.pth"
        if model_path.exists():
            eval_result = system.evaluate_model(str(model_path))
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        system.save_training_report()
        
        print("\n" + "="*70)
        print("âœ… TRAINING COMPLETE!")
        print("="*70)
        print("\nğŸ“Š Summary:")
        print(f"   Training samples: {num_samples}")
        print(f"   Epochs: {system.training_stats['epochs_completed']}")
        print(f"   Best loss: {system.training_stats['best_loss']:.4f}")
        print(f"   Training time: {system.training_stats['training_time']:.2f}s")
        print(f"\nğŸ’¡ Model ready for inference!")
        print(f"   Use: python -m cad3d.vit_integration")
    else:
        print("\nâŒ Training failed")
        print(f"   Error: {result.get('error', 'Unknown error')}")


if __name__ == "__main__":
    main()
